#!/usr/bin/env python3
# lstm_dataset_processor.py - Prepare agricultural IoT sensor data for LSTM analysis
# This processor creates both labeled and unlabeled datasets, generates sequences,
# and saves all necessary files for LSTM model validation

import json
import pandas as pd
import numpy as np
import argparse
import os
from pathlib import Path
import matplotlib.pyplot as plt
import joblib
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Import shared utilities for consistency with other processors
from shared_metrics_utils import load_jsonl, extract_metrics, calculate_derived_metrics

def load_dataset_from_metadata(metadata_file):
    """
    Load and process all data specified in the metadata file
    generated by lstm_dataset_generator.sh
    """
    print(f"Loading metadata from: {metadata_file}")
    try:
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
    except Exception as e:
        print(f"Error loading metadata file: {e}")
        return None
    
    # Prepare to collect all datasets
    all_scenarios = []
    
    # Process each scenario in the metadata
    for i, scenario in enumerate(metadata.get("scenarios", [])):
        scenario_type = scenario.get("type", "unknown")
        fault_type = scenario.get("fault_type", "unknown")
        data_file = scenario.get("data_file", None)
        
        print(f"Processing scenario {i+1}/{len(metadata.get('scenarios', []))}: {scenario_type} with {fault_type} fault")
        
        if not data_file or not os.path.exists(data_file):
            print(f"WARNING: Data file {data_file} not found. Skipping.")
            continue
        
        # Load the data from JSONL file
        jsonl_data = load_jsonl(data_file)
        if not jsonl_data:
            print(f"WARNING: No data found in {data_file}. Skipping.")
            continue
        
        # Extract metrics from the data
        # For attack data, use 'event' as the phase
        # For normal data, use 'baseline' as the phase
        phase = "event" if scenario_type != "normal" else "baseline"
        metrics = extract_metrics(jsonl_data, phase, fault_type)
        
        if not metrics:
            print(f"WARNING: Failed to extract metrics from {data_file}. Skipping.")
            continue
        
        # Create a DataFrame and add scenario information
        scenario_df = pd.DataFrame(metrics)
        
        # Add attack type information
        scenario_df["attack_type"] = "no_attack" if scenario_type == "normal" else scenario_type
        
        # Add binary attack flag (1 for attack, 0 for normal)
        scenario_df["is_attack"] = 1 if scenario_type != "normal" else 0
        
        # Add scenario ID for tracking
        scenario_id = f"scenario_{i+1}_{scenario_type}_{fault_type}"
        scenario_df["scenario_id"] = scenario_id
        
        # Calculate derived metrics
        scenario_df = calculate_derived_metrics(scenario_df)
        
        all_scenarios.append({
            "id": scenario_id,
            "type": scenario_type,
            "fault_type": fault_type,
            "dataframe": scenario_df,
            "num_records": len(scenario_df)
        })
    
    return all_scenarios

def create_segment(df, segment_id, segment_length=120, step_size=30, is_unlabeled=False):
    """
    Create a segment of data for LSTM sequence generation
    
    Parameters:
    - df: DataFrame containing a full dataset
    - segment_id: Unique identifier for this segment
    - segment_length: Number of records to include in the segment
    - step_size: Number of records to step when creating overlapping segments
    - is_unlabeled: If True, remove the labels for test purposes
    
    Returns:
    - list of segments, each a DataFrame with a segment_id column
    """
    segments = []
    
    # Make sure we have enough data
    if len(df) < segment_length:
        print(f"WARNING: Dataframe too short for segmentation: {len(df)} records. Skipping.")
        return segments
    
    # Sort by timestamp to ensure sequence validity
    df = df.sort_values("timestamp").reset_index(drop=True)
    
    # Generate overlapping segments
    for start_idx in range(0, len(df) - segment_length + 1, step_size):
        end_idx = start_idx + segment_length
        segment_df = df.iloc[start_idx:end_idx].copy()
        
        # Add segment ID
        unique_segment_id = f"{segment_id}_seg_{start_idx // step_size + 1}"
        segment_df["segment_id"] = unique_segment_id
        
        # If this is for unlabeled testing, remove the label information
        if is_unlabeled:
            # We'll keep attack_type but remove is_attack flag
            # This allows us to verify our results later but prevents the LSTM
            # from using the label during training
            segment_df["is_unlabeled"] = 1
        else:
            segment_df["is_unlabeled"] = 0
        
        # Extract scenario type from scenario_id if possible
        scenario_type = "unknown"
        try:
            scenario_type = df["scenario_id"].iloc[0].split("_")[2]
        except (IndexError, KeyError):
            pass
        
        segments.append({
            "segment_id": unique_segment_id,
            "dataframe": segment_df,
            "scenario_type": scenario_type,
            "fault_type": df["vulnerability_type"].iloc[0] if "vulnerability_type" in df.columns else "unknown",
            "attack_type": df["attack_type"].iloc[0] if "attack_type" in df.columns else "unknown",
            "is_unlabeled": is_unlabeled,
            "num_records": len(segment_df)
        })
    
    return segments

def prepare_lstm_sequences(segments, feature_columns, sequence_length=30):
    """
    Prepare sequence data for LSTM model training and testing
    
    Parameters:
    - segments: List of data segments
    - feature_columns: List of column names to use as features
    - sequence_length: Length of each LSTM sequence
    
    Returns:
    - Dictionary with X_train, y_train, X_test, y_test, and more
    """
    print(f"Preparing LSTM sequences with length {sequence_length}")
    
    # First, consolidate all segment data
    labeled_data = []
    unlabeled_data = []
    segments_info = []
    
    for segment in segments:
        # Extract basic segment info
        segment_info = {
            "segment_id": segment["segment_id"],
            "scenario_type": segment["scenario_type"],
            "fault_type": segment["fault_type"],
            "attack_type": segment["attack_type"],
            "is_unlabeled": segment["is_unlabeled"],
            "num_records": segment["num_records"]
        }
        segments_info.append(segment_info)
        
        # Add data to appropriate collection
        if segment["is_unlabeled"]:
            unlabeled_data.append(segment["dataframe"])
        else:
            labeled_data.append(segment["dataframe"])
    
    # Create DataFrames
    labeled_df = pd.concat(labeled_data) if labeled_data else pd.DataFrame()
    unlabeled_df = pd.concat(unlabeled_data) if unlabeled_data else pd.DataFrame()
    segments_info_df = pd.DataFrame(segments_info)
    
    print(f"Created labeled dataset with {len(labeled_df)} records")
    print(f"Created unlabeled dataset with {len(unlabeled_df)} records")
    print(f"Collected info on {len(segments_info)} segments")
    
    # First, verify feature columns exist in the labeled DataFrame
    validated_features = []
    for col in feature_columns:
        if col in labeled_df.columns:
            validated_features.append(col)
        else:
            print(f"WARNING: Feature column '{col}' not found in dataset. Skipping.")
    
    if not validated_features:
        print("ERROR: No valid feature columns found. Using fallback approach.")
        # Fallback: try to find any numeric columns that might work
        numeric_cols = labeled_df.select_dtypes(include=[np.number]).columns.tolist()
        # Filter out unwanted columns
        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'is_attack', 'is_unlabeled']]
        if numeric_cols:
            print(f"Using {len(numeric_cols)} numeric columns as features.")
            validated_features = numeric_cols[:min(6, len(numeric_cols))]  # Limit to 6 features max
        else:
            print("ERROR: No numeric columns found. Cannot proceed.")
            return None
    
    print(f"Using {len(validated_features)} validated feature columns")
    
    # Handle missing values
    labeled_df[validated_features] = labeled_df[validated_features].fillna(0)
    if not unlabeled_df.empty:
        # Add any missing columns to unlabeled data with zero values
        for col in validated_features:
            if col not in unlabeled_df.columns:
                unlabeled_df[col] = 0
        unlabeled_df[validated_features] = unlabeled_df[validated_features].fillna(0)
    
    # Scale the features
    scaler = MinMaxScaler(feature_range=(0, 1))
    labeled_features = scaler.fit_transform(labeled_df[validated_features])
    
    # Create sequences for LSTM
    X_sequences = []
    y_sequences = []
    
    # Make sure we have enough data for at least one sequence
    if len(labeled_features) <= sequence_length:
        print(f"WARNING: Not enough data for sequences. Have {len(labeled_features)} records, need > {sequence_length}")
        if len(labeled_features) > 0:
            # Reduce sequence length as a fallback
            new_seq_length = max(5, len(labeled_features) // 2)
            print(f"Reducing sequence length to {new_seq_length}")
            sequence_length = new_seq_length
        else:
            print("ERROR: No labeled data available. Cannot create sequences.")
            return None
    
    for i in range(len(labeled_features) - sequence_length):
        X_sequences.append(labeled_features[i:i+sequence_length])
        y_sequences.append(labeled_df["is_attack"].iloc[i+sequence_length])
    
    X_sequences = np.array(X_sequences)
    y_sequences = np.array(y_sequences)
    
    print(f"Created {len(X_sequences)} sequences of length {sequence_length}")
    
    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        X_sequences, y_sequences, test_size=0.3, random_state=42, shuffle=True
    )
    
    return {
        "X_train": X_train,
        "X_test": X_test,
        "y_train": y_train,
        "y_test": y_test,
        "labeled_df": labeled_df,
        "unlabeled_df": unlabeled_df,
        "segments_info_df": segments_info_df,
        "feature_columns": validated_features,
        "scaler": scaler,
        "sequence_length": sequence_length
    }

def identify_feature_columns(df):
    """
    Identify and select the appropriate feature columns for LSTM modeling
    """
    # Print all available columns for debugging
    print("Available columns:")
    for col in df.columns:
        print(f"  {col}")
    
    # Key categories of features we want to include
    cpu_cols = [col for col in df.columns if col.startswith("cpu_") and not col.endswith("_total") and not col.endswith("_roll_mean") and not col.endswith("_roll_std") and not col.endswith("_zscore")]
    memory_cols = [col for col in df.columns if col.startswith("memory_") and not col.endswith("_total")]
    temp_dev_cols = [col for col in df.columns if "true_dev_" in col and not col.endswith("_roll_mean") and not col.endswith("_roll_std") and not col.endswith("_zscore")]
    reporting_interval_cols = [col for col in df.columns if col.startswith("reporting_interval_")]
    latency_cols = [col for col in df.columns if col.startswith("latency_ms_") and not col.endswith("_estimated") and not col.endswith("_method")]
    network_rate_cols = [col for col in df.columns if "network_sent_rate_" in col or "network_received_rate_" in col]
    
    # Print identified category columns for debugging
    print("\nIdentified column categories:")
    print(f"  CPU columns: {cpu_cols}")
    print(f"  Memory columns: {memory_cols}")
    print(f"  Temperature deviation columns: {temp_dev_cols}")
    print(f"  Reporting interval columns: {reporting_interval_cols}")
    print(f"  Latency columns: {latency_cols}")
    print(f"  Network rate columns: {network_rate_cols}")
    
    # Select one column from each category (if available)
    selected_cols = []
    
    if cpu_cols:
        selected_cols.append(cpu_cols[0])
    
    if memory_cols:
        selected_cols.append(memory_cols[0])
    
    if temp_dev_cols:
        selected_cols.append(temp_dev_cols[0])
    
    if reporting_interval_cols:
        selected_cols.append(reporting_interval_cols[0])
    
    if latency_cols:
        selected_cols.append(latency_cols[0])
    
    if network_rate_cols:
        selected_cols.append(network_rate_cols[0])
    
    # If no columns were found, try a more generic approach
    if not selected_cols:
        print("WARNING: Could not identify specific feature columns using standard patterns")
        # Look for any numeric columns that might be useful
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        # Exclude non-feature columns
        exclude_patterns = ["timestamp", "human_time", "is_attack", "segment_id", "is_unlabeled", "_method", "_estimated"]
        numeric_cols = [col for col in numeric_cols if not any(pattern in col for pattern in exclude_patterns)]
        if numeric_cols:
            # Take up to 6 numeric columns
            selected_cols = numeric_cols[:min(6, len(numeric_cols))]
            print(f"Using {len(selected_cols)} numeric columns as features instead")
    
    if not selected_cols:
        print("ERROR: Could not identify any feature columns")
        # Last resort: use any numeric columns except timestamp
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        selected_cols = [col for col in numeric_cols if col != "timestamp"][:min(4, len(numeric_cols))]
        if not selected_cols:
            print("CRITICAL ERROR: No numeric columns found for features")
            return []
    
    print(f"\nSelected {len(selected_cols)} feature columns:")
    for col in selected_cols:
        print(f"  {col}")
    
    return selected_cols

def save_lstm_data(lstm_data, output_dir):
    """
    Save all LSTM-ready data to the specified output directory
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    print(f"Saving LSTM data to {output_dir}")
    
    # Save numpy arrays
    np.save(output_dir / "X_train.npy", lstm_data["X_train"])
    np.save(output_dir / "X_test.npy", lstm_data["X_test"])
    np.save(output_dir / "y_train.npy", lstm_data["y_train"])
    np.save(output_dir / "y_test.npy", lstm_data["y_test"])
    
    # Save DataFrames
    lstm_data["labeled_df"].to_csv(output_dir / "labeled_data.csv", index=False)
    lstm_data["segments_info_df"].to_csv(output_dir / "segments_info.csv", index=False)
    if not lstm_data["unlabeled_df"].empty:
        lstm_data["unlabeled_df"].to_csv(output_dir / "unlabeled_data.csv", index=False)
    
    # Combine all data for exploration
    all_data = pd.concat([lstm_data["labeled_df"], lstm_data["unlabeled_df"]])
    all_data.to_csv(output_dir / "raw_combined_data.csv", index=False)
    
    # Save feature columns
    with open(output_dir / "feature_columns.json", "w") as f:
        json.dump(lstm_data["feature_columns"], f)
    
    # Save scaler
    joblib.dump(lstm_data["scaler"], output_dir / "feature_scaler.joblib")
    
    # Save dataset info
    dataset_info = {
        "total_records": len(lstm_data["labeled_df"]) + len(lstm_data["unlabeled_df"]),
        "labeled_records": len(lstm_data["labeled_df"]),
        "unlabeled_records": len(lstm_data["unlabeled_df"]),
        "training_sequences": len(lstm_data["X_train"]),
        "testing_sequences": len(lstm_data["X_test"]),
        "sequence_length": lstm_data["sequence_length"],
        "feature_count": len(lstm_data["feature_columns"]),
        "attack_distribution": lstm_data["labeled_df"]["attack_type"].value_counts().to_dict(),
        "fault_distribution": lstm_data["labeled_df"]["vulnerability_type"].value_counts().to_dict(),
    }
    
    with open(output_dir / "dataset_info.json", "w") as f:
        json.dump(dataset_info, f, indent=2)
    
    print("Successfully saved all LSTM data files")

def visualize_dataset(lstm_data, output_dir):
    """
    Generate exploratory visualizations of the dataset
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    labeled_df = lstm_data["labeled_df"]
    
    # Skip visualizations if no data
    if labeled_df.empty:
        print("No data available for visualization")
        return
    
    try:
        # 1. Distribution of attack types
        plt.figure(figsize=(10, 6))
        if "attack_type" in labeled_df.columns:
            attack_counts = labeled_df["attack_type"].value_counts()
            plt.bar(attack_counts.index, attack_counts.values)
            plt.title("Distribution of Attack Types")
            plt.xlabel("Attack Type")
            plt.ylabel("Count")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(output_dir / "attack_distribution.png")
        plt.close()
        
        # 2. Distribution of fault types
        plt.figure(figsize=(10, 6))
        if "vulnerability_type" in labeled_df.columns:
            fault_counts = labeled_df["vulnerability_type"].value_counts()
            plt.bar(fault_counts.index, fault_counts.values)
            plt.title("Distribution of Fault Types")
            plt.xlabel("Fault Type")
            plt.ylabel("Count")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(output_dir / "fault_distribution.png")
        plt.close()
        
        # 3. Attack vs Non-attack distribution
        plt.figure(figsize=(8, 6))
        if "is_attack" in labeled_df.columns:
            attack_binary = labeled_df["is_attack"].value_counts()
            plt.bar(["No Attack", "Attack"], [attack_binary.get(0, 0), attack_binary.get(1, 0)])
            plt.title("Attack vs. Non-Attack Distribution")
            plt.ylabel("Count")
            plt.tight_layout()
            plt.savefig(output_dir / "attack_binary_distribution.png")
        plt.close()
        
        # 4. Feature distributions by attack type
        feature_cols = lstm_data["feature_columns"]
        
        for col in feature_cols:
            if col not in labeled_df.columns:
                continue
                
            plt.figure(figsize=(12, 8))
            if "attack_type" in labeled_df.columns:
                for attack_type in labeled_df["attack_type"].unique():
                    attack_data = labeled_df[labeled_df["attack_type"] == attack_type][col].dropna()
                    if not attack_data.empty:
                        plt.hist(attack_data, alpha=0.5, bins=30, label=attack_type)
            
            plt.title(f"Distribution of {col} by Attack Type")
            plt.xlabel(col)
            plt.ylabel("Frequency")
            plt.legend()
            plt.tight_layout()
            safe_col_name = col.replace('_', '').replace('-', '').replace('.', '')
            plt.savefig(output_dir / f"feature_dist_{safe_col_name}.png")
            plt.close()
        
        print(f"Generated visualizations saved to {output_dir}")
    except Exception as e:
        print(f"Error during visualization: {e}")

def create_unlabeled_segments(scenarios, unlabeled_ratio=0.2):
    """
    Designate some scenarios as "unlabeled" for testing purposes
    
    Parameters:
    - scenarios: List of scenario data
    - unlabeled_ratio: Proportion of scenarios to mark as unlabeled
    
    Returns:
    - Two lists: labeled_scenarios and unlabeled_scenarios
    """
    if not scenarios:
        print("No scenarios provided")
        return [], []
        
    # Shuffle the scenarios for random selection
    import random
    random.seed(42)  # For reproducibility
    shuffled = scenarios.copy()
    random.shuffle(shuffled)
    
    # Calculate number of scenarios to mark as unlabeled
    n_unlabeled = max(1, int(len(shuffled) * unlabeled_ratio))
    
    # Split into labeled and unlabeled
    unlabeled_scenarios = shuffled[:n_unlabeled]
    labeled_scenarios = shuffled[n_unlabeled:]
    
    print(f"Created {len(labeled_scenarios)} labeled scenarios and {len(unlabeled_scenarios)} unlabeled scenarios")
    
    return labeled_scenarios, unlabeled_scenarios

def main():
    parser = argparse.ArgumentParser(description="Process datasets for LSTM model validation")
    parser.add_argument("--metadata", required=True, help="Master metadata JSON file from lstm_dataset_generator.sh")
    parser.add_argument("--output", default="analysis/lstm", help="Output directory for LSTM-ready data")
    parser.add_argument("--seq-length", type=int, default=30, help="Sequence length for LSTM input")
    parser.add_argument("--segment-length", type=int, default=120, help="Length of each data segment")
    parser.add_argument("--unlabeled-ratio", type=float, default=0.2, help="Ratio of data to use as unlabeled")
    parser.add_argument("--step-size", type=int, default=30, help="Step size for creating overlapping segments")
    parser.add_argument("--debug", action="store_true", help="Enable debug output")
    
    args = parser.parse_args()
    
    # Create output directory
    output_dir = Path(args.output)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    try:
        # Load and process the data from metadata
        scenarios = load_dataset_from_metadata(args.metadata)
        
        if not scenarios:
            print("Error: Failed to load scenarios from metadata")
            return
        
        print(f"Loaded {len(scenarios)} scenarios")
        
        # Designate some scenarios as unlabeled for testing
        labeled_scenarios, unlabeled_scenarios = create_unlabeled_segments(scenarios, args.unlabeled_ratio)
        
        # Determine feature columns from the first scenario's dataframe
        if labeled_scenarios:
            feature_columns = identify_feature_columns(labeled_scenarios[0]["dataframe"])
        else:
            print("Error: No labeled scenarios available")
            return
        
        # Create segments for sequence generation
        all_segments = []
        
        print("Creating labeled segments...")
        for scenario in labeled_scenarios:
            segments = create_segment(
                scenario["dataframe"], 
                scenario["id"],
                segment_length=args.segment_length,
                step_size=args.step_size,
                is_unlabeled=False
            )
            all_segments.extend(segments)
        
        print("Creating unlabeled segments...")
        for scenario in unlabeled_scenarios:
            segments = create_segment(
                scenario["dataframe"], 
                scenario["id"],
                segment_length=args.segment_length,
                step_size=args.step_size,
                is_unlabeled=True
            )
            all_segments.extend(segments)
        
        print(f"Created a total of {len(all_segments)} segments")
        
        # Prepare sequences for LSTM
        lstm_data = prepare_lstm_sequences(
            all_segments, 
            feature_columns, 
            sequence_length=args.seq_length
        )
        
        if not lstm_data:
            print("Error: Failed to prepare LSTM sequences")
            return
        
        # Save all the prepared data
        save_lstm_data(lstm_data, output_dir)
        
        # Create visualizations
        visualize_dataset(lstm_data, output_dir)
        
        print(f"LSTM data processing complete. Results saved to {output_dir}")
        print("You can now run the LSTM validation notebook on this data.")
        
    except Exception as e:
        print(f"Error during data processing: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
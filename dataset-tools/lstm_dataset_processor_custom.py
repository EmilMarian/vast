#!/usr/bin/env python3
# lstm_dataset_processor_custom.py - Prepare agricultural IoT sensor data for LSTM analysis
# Customized version to handle smaller datasets with flexible segmentation

import json
import pandas as pd
import numpy as np
import argparse
import os
from pathlib import Path
import matplotlib.pyplot as plt
import joblib
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Import shared utilities for consistency with other processors
from shared_metrics_utils import load_jsonl, extract_metrics, calculate_derived_metrics

def load_dataset_from_metadata(metadata_file):
    """
    Load and process all data specified in the metadata file
    generated by lstm_dataset_generator.sh
    """
    print(f"Loading metadata from: {metadata_file}")
    try:
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
    except Exception as e:
        print(f"Error loading metadata file: {e}")
        return None
    
    # Prepare to collect all datasets
    all_scenarios = []
    
    # Process each scenario in the metadata
    for i, scenario in enumerate(metadata.get("scenarios", [])):
        scenario_type = scenario.get("type", "unknown")
        fault_type = scenario.get("fault_type", "unknown")
        data_file = scenario.get("data_file", None)
        
        print(f"Processing scenario {i+1}/{len(metadata.get('scenarios', []))}: {scenario_type} with {fault_type} fault")
        
        if not data_file or not os.path.exists(data_file):
            print(f"WARNING: Data file {data_file} not found. Skipping.")
            continue
        
        # Load the data from JSONL file
        jsonl_data = load_jsonl(data_file)
        if not jsonl_data:
            print(f"WARNING: No data found in {data_file}. Skipping.")
            continue
        
        # Extract metrics from the data
        # For attack data, use 'event' as the phase
        # For normal data, use 'baseline' as the phase
        phase = "event" if scenario_type != "normal" else "baseline"
        metrics = extract_metrics(jsonl_data, phase, fault_type)
        
        if not metrics:
            print(f"WARNING: Failed to extract metrics from {data_file}. Skipping.")
            continue
        
        # Create a DataFrame and add scenario information
        scenario_df = pd.DataFrame(metrics)
        
        # Add attack type information
        scenario_df["attack_type"] = "no_attack" if scenario_type == "normal" else scenario_type
        
        # Add binary attack flag (1 for attack, 0 for normal)
        scenario_df["is_attack"] = 1 if scenario_type != "normal" else 0
        
        # Add scenario ID for tracking
        scenario_id = f"scenario_{i+1}_{scenario_type}_{fault_type}"
        scenario_df["scenario_id"] = scenario_id
        
        # Calculate derived metrics
        scenario_df = calculate_derived_metrics(scenario_df)
        
        # Print data size for debugging
        print(f"  Loaded {len(scenario_df)} records for {scenario_id}")
        
        all_scenarios.append({
            "id": scenario_id,
            "type": scenario_type,
            "fault_type": fault_type,
            "dataframe": scenario_df,
            "num_records": len(scenario_df)
        })
    
    return all_scenarios

def identify_feature_columns(df, sensor_id="temperature-sensor-04"):
    """
    Identify and select the appropriate feature columns for LSTM modeling
    Specifically targeting the specified sensor ID
    """
    # Print all available columns for debugging
    print("Available columns:")
    for col in df.columns:
        print(f"  {col}")
    
    # Key categories of features we want to include for the specified sensor
    cpu_cols = [col for col in df.columns if col.startswith("cpu_" + sensor_id)]
    memory_cols = [col for col in df.columns if col.startswith("memory_" + sensor_id)]
    temp_dev_cols = [col for col in df.columns if "true_dev_" + sensor_id in col]
    reporting_interval_cols = [col for col in df.columns if col.startswith("reporting_interval_" + sensor_id)]
    latency_cols = [col for col in df.columns if col.startswith("latency_ms_temperature_" + sensor_id)]
    network_rate_cols = [col for col in df.columns if ("network_sent_rate_" + sensor_id in col) or ("network_received_rate_" + sensor_id in col)]
    
    # Print identified category columns for debugging
    print(f"\nIdentified {sensor_id} column categories:")
    print(f"  CPU columns: {cpu_cols}")
    print(f"  Memory columns: {memory_cols}")
    print(f"  Temperature deviation columns: {temp_dev_cols}")
    print(f"  Reporting interval columns: {reporting_interval_cols}")
    print(f"  Latency columns: {latency_cols}")
    print(f"  Network rate columns: {network_rate_cols}")
    
    # Select one column from each category (if available)
    selected_cols = []
    
    if cpu_cols:
        selected_cols.append(cpu_cols[0])
    
    if memory_cols:
        selected_cols.append(memory_cols[0])
    
    if temp_dev_cols:
        selected_cols.append(temp_dev_cols[0])
    
    if reporting_interval_cols:
        selected_cols.append(reporting_interval_cols[0])
    
    if latency_cols:
        selected_cols.append(latency_cols[0])
    
    if network_rate_cols:
        selected_cols.append(network_rate_cols[0])
    
    # If no columns were found for the specified sensor, fall back to the default approach
    if not selected_cols:
        print(f"WARNING: Could not identify specific feature columns for {sensor_id}")
        # Look for any numeric columns that might be useful
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        # Exclude non-feature columns
        exclude_patterns = ["timestamp", "human_time", "is_attack", "segment_id", "is_unlabeled", "_method", "_estimated"]
        numeric_cols = [col for col in numeric_cols if not any(pattern in col for pattern in exclude_patterns)]
        if numeric_cols:
            # Take up to 6 numeric columns
            selected_cols = numeric_cols[:min(6, len(numeric_cols))]
            print(f"Using {len(selected_cols)} generic numeric columns as features instead")
    
    if not selected_cols:
        print(f"ERROR: Could not identify any feature columns for {sensor_id}")
        # Last resort: use any numeric columns except timestamp
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        selected_cols = [col for col in numeric_cols if col != "timestamp"][:min(4, len(numeric_cols))]
        if not selected_cols:
            print("CRITICAL ERROR: No numeric columns found for features")
            return []
    
    print(f"\nSelected {len(selected_cols)} feature columns for {sensor_id}:")
    for col in selected_cols:
        print(f"  {col}")
    
    return selected_cols

def prepare_direct_lstm_data(scenarios, feature_columns, test_size=0.3, sequence_length=10):
    """
    Prepare LSTM data directly from scenarios without segmentation
    
    This approach treats each scenario as its own chunk of data and
    creates sequences from it directly, avoiding the segmentation step
    that was causing issues with small datasets.
    """
    print(f"Preparing LSTM sequences with length {sequence_length} directly from scenarios")
    
    # First, collect all data
    labeled_data = []
    unlabeled_data = []
    segments_info = []
    
    # Determine which scenarios to use for testing (unlabeled)
    import random
    random.seed(42)  # For reproducibility
    
    n_unlabeled = max(1, int(len(scenarios) * 0.2))  # Use 20% for unlabeled data
    unlabeled_indices = random.sample(range(len(scenarios)), n_unlabeled)
    
    for i, scenario in enumerate(scenarios):
        # Create segment info
        segment_info = {
            "segment_id": scenario["id"],
            "scenario_type": scenario["type"],
            "fault_type": scenario["fault_type"],
            "attack_type": "no_attack" if scenario["type"] == "normal" else scenario["type"],
            "is_unlabeled": i in unlabeled_indices,
            "num_records": len(scenario["dataframe"])
        }
        segments_info.append(segment_info)
        
        # Mark as unlabeled if selected for testing
        if i in unlabeled_indices:
            df_copy = scenario["dataframe"].copy()
            df_copy["is_unlabeled"] = 1
            unlabeled_data.append(df_copy)
        else:
            df_copy = scenario["dataframe"].copy()
            df_copy["is_unlabeled"] = 0
            labeled_data.append(df_copy)
    
    # Combine data
    labeled_df = pd.concat(labeled_data) if labeled_data else pd.DataFrame()
    unlabeled_df = pd.concat(unlabeled_data) if unlabeled_data else pd.DataFrame()
    segments_info_df = pd.DataFrame(segments_info)
    
    print(f"Created labeled dataset with {len(labeled_df)} records")
    print(f"Created unlabeled dataset with {len(unlabeled_df)} records")
    
    if labeled_df.empty:
        print("ERROR: No labeled data available")
        return None
    
    # Validate feature columns exist in the dataset
    valid_features = []
    for col in feature_columns:
        if col in labeled_df.columns:
            valid_features.append(col)
        else:
            print(f"WARNING: Feature column '{col}' not found in dataset. Skipping.")
    
    if not valid_features:
        print("ERROR: No valid feature columns. Trying to find numeric columns...")
        # Find numeric columns as a fallback
        numeric_cols = labeled_df.select_dtypes(include=[np.number]).columns.tolist()
        # Filter out unwanted columns
        exclude_patterns = ["timestamp", "human_time", "is_attack", "segment_id", "is_unlabeled", "_method", "_estimated"]
        valid_features = [col for col in numeric_cols if not any(pattern in col for pattern in exclude_patterns)][:min(6, len(numeric_cols))]
        
        if not valid_features:
            print("CRITICAL ERROR: Could not find any usable feature columns")
            return None
    
    print(f"Using {len(valid_features)} valid feature columns for LSTM")
    for col in valid_features:
        print(f"  {col}")
    
    # Fill missing values
    labeled_df[valid_features] = labeled_df[valid_features].fillna(0)
    
    # Scale the features
    scaler = MinMaxScaler(feature_range=(0, 1))
    labeled_features = scaler.fit_transform(labeled_df[valid_features])
    
    # Prepare sequences for each scenario separately, then combine
    X_all = []
    y_all = []
    
    # Create a scenario ID column for grouping
    for scenario_id in labeled_df["scenario_id"].unique():
        # Get data for this scenario
        scenario_data = labeled_df[labeled_df["scenario_id"] == scenario_id]
        scenario_data = scenario_data.sort_values("timestamp").reset_index(drop=True)
        
        # If sequence_length is too big for this scenario, use a smaller value
        effective_seq_length = min(sequence_length, len(scenario_data) // 2)
        if effective_seq_length < 5:  # Too small to be useful
            print(f"WARNING: Scenario {scenario_id} has too few records ({len(scenario_data)}) for meaningful sequences")
            continue
            
        # Get features for this scenario
        scenario_features = scaler.transform(scenario_data[valid_features])
        
        # Create sequences
        for i in range(len(scenario_features) - effective_seq_length):
            X_all.append(scenario_features[i:i+effective_seq_length])
            y_all.append(scenario_data["is_attack"].iloc[i+effective_seq_length])
    
    if not X_all:
        print("ERROR: Could not create any sequences from the data")
        return None
        
    X_all = np.array(X_all)
    y_all = np.array(y_all)
    
    print(f"Created {len(X_all)} sequences in total")
    
    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        X_all, y_all, test_size=test_size, random_state=42, shuffle=True
    )
    
    # Return all prepared data
    return {
        "X_train": X_train,
        "X_test": X_test,
        "y_train": y_train,
        "y_test": y_test,
        "labeled_df": labeled_df,
        "unlabeled_df": unlabeled_df,
        "segments_info_df": segments_info_df,
        "feature_columns": valid_features,
        "scaler": scaler,
        "sequence_length": sequence_length
    }

def save_lstm_data(lstm_data, output_dir):
    """
    Save all LSTM-ready data to the specified output directory
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    print(f"Saving LSTM data to {output_dir}")
    
    # Save numpy arrays
    np.save(output_dir / "X_train.npy", lstm_data["X_train"])
    np.save(output_dir / "X_test.npy", lstm_data["X_test"])
    np.save(output_dir / "y_train.npy", lstm_data["y_train"])
    np.save(output_dir / "y_test.npy", lstm_data["y_test"])
    
    # Save DataFrames
    lstm_data["labeled_df"].to_csv(output_dir / "labeled_data.csv", index=False)
    lstm_data["segments_info_df"].to_csv(output_dir / "segments_info.csv", index=False)
    if not lstm_data["unlabeled_df"].empty:
        lstm_data["unlabeled_df"].to_csv(output_dir / "unlabeled_data.csv", index=False)
    
    # Combine all data for exploration
    all_data = pd.concat([lstm_data["labeled_df"], lstm_data["unlabeled_df"]])
    all_data.to_csv(output_dir / "raw_combined_data.csv", index=False)
    
    # Save feature columns
    with open(output_dir / "feature_columns.json", "w") as f:
        json.dump(lstm_data["feature_columns"], f)
    
    # Save scaler
    joblib.dump(lstm_data["scaler"], output_dir / "feature_scaler.joblib")
    
    # Save dataset info
    dataset_info = {
        "total_records": len(lstm_data["labeled_df"]) + len(lstm_data["unlabeled_df"]),
        "labeled_records": len(lstm_data["labeled_df"]),
        "unlabeled_records": len(lstm_data["unlabeled_df"]),
        "training_sequences": len(lstm_data["X_train"]),
        "testing_sequences": len(lstm_data["X_test"]),
        "sequence_length": lstm_data["sequence_length"],
        "feature_count": len(lstm_data["feature_columns"]),
        "attack_distribution": lstm_data["labeled_df"]["attack_type"].value_counts().to_dict(),
        "fault_distribution": lstm_data["labeled_df"]["vulnerability_type"].value_counts().to_dict(),
    }
    
    with open(output_dir / "dataset_info.json", "w") as f:
        json.dump(dataset_info, f, indent=2)
    
    print("Successfully saved all LSTM data files")

def visualize_dataset(lstm_data, output_dir):
    """
    Generate exploratory visualizations of the dataset
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    labeled_df = lstm_data["labeled_df"]
    
    # Skip visualizations if no data
    if labeled_df.empty:
        print("No data available for visualization")
        return
    
    try:
        # 1. Distribution of attack types
        plt.figure(figsize=(10, 6))
        if "attack_type" in labeled_df.columns:
            attack_counts = labeled_df["attack_type"].value_counts()
            plt.bar(attack_counts.index, attack_counts.values)
            plt.title("Distribution of Attack Types")
            plt.xlabel("Attack Type")
            plt.ylabel("Count")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(output_dir / "attack_distribution.png")
        plt.close()
        
        # 2. Distribution of fault types
        plt.figure(figsize=(10, 6))
        if "vulnerability_type" in labeled_df.columns:
            fault_counts = labeled_df["vulnerability_type"].value_counts()
            plt.bar(fault_counts.index, fault_counts.values)
            plt.title("Distribution of Fault Types")
            plt.xlabel("Fault Type")
            plt.ylabel("Count")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(output_dir / "fault_distribution.png")
        plt.close()
        
        # 3. Attack vs Non-attack distribution
        plt.figure(figsize=(8, 6))
        if "is_attack" in labeled_df.columns:
            attack_binary = labeled_df["is_attack"].value_counts()
            plt.bar(["No Attack", "Attack"], [attack_binary.get(0, 0), attack_binary.get(1, 0)])
            plt.title("Attack vs. Non-Attack Distribution")
            plt.ylabel("Count")
            plt.tight_layout()
            plt.savefig(output_dir / "attack_binary_distribution.png")
        plt.close()
        
        # 4. Feature distributions by attack type
        feature_cols = lstm_data["feature_columns"]
        
        for col in feature_cols:
            if col not in labeled_df.columns:
                continue
                
            plt.figure(figsize=(12, 8))
            if "attack_type" in labeled_df.columns:
                for attack_type in labeled_df["attack_type"].unique():
                    attack_data = labeled_df[labeled_df["attack_type"] == attack_type][col].dropna()
                    if not attack_data.empty:
                        plt.hist(attack_data, alpha=0.5, bins=30, label=attack_type)
            
            plt.title(f"Distribution of {col} by Attack Type")
            plt.xlabel(col)
            plt.ylabel("Frequency")
            plt.legend()
            plt.tight_layout()
            safe_col_name = col.replace('_', '').replace('-', '').replace('.', '')
            plt.savefig(output_dir / f"feature_dist_{safe_col_name}.png")
            plt.close()
        
        print(f"Generated visualizations saved to {output_dir}")
    except Exception as e:
        print(f"Error during visualization: {e}")

def main():
    parser = argparse.ArgumentParser(description="Process datasets for LSTM model validation")
    parser.add_argument("--metadata", required=True, help="Master metadata JSON file from lstm_dataset_generator.sh")
    parser.add_argument("--output", default="analysis/lstm", help="Output directory for LSTM-ready data")
    parser.add_argument("--seq-length", type=int, default=10, help="Sequence length for LSTM input (default: 10)")
    parser.add_argument("--test-size", type=float, default=0.3, help="Proportion of data to use for testing (default: 0.3)")
    parser.add_argument("--debug", action="store_true", help="Enable debug output")
    
    args = parser.parse_args()
    
    # Create output directory
    output_dir = Path(args.output)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    try:
        # Load and process the data from metadata
        scenarios = load_dataset_from_metadata(args.metadata)
        
        if not scenarios:
            print("Error: Failed to load scenarios from metadata")
            return
        
        print(f"Loaded {len(scenarios)} scenarios")
        
        # Determine feature columns from the first scenario's dataframe
        # Inside the main() function, find where it calls identify_feature_columns and change it to:
        if scenarios:
            feature_columns = identify_feature_columns(scenarios[0]["dataframe"], "temperature-sensor-04")
        else:
            print("Error: No scenarios available")
            return
        
        # Prepare LSTM data directly from scenarios
        lstm_data = prepare_direct_lstm_data(
            scenarios,
            feature_columns,
            test_size=args.test_size,
            sequence_length=args.seq_length
        )
        
        if not lstm_data:
            print("Error: Failed to prepare LSTM data")
            return
        
        # Save all the prepared data
        save_lstm_data(lstm_data, output_dir)
        
        # Create visualizations
        visualize_dataset(lstm_data, output_dir)
        
        print(f"LSTM data processing complete. Results saved to {output_dir}")
        print("You can now run the LSTM validation notebook on this data.")
        
    except Exception as e:
        print(f"Error during data processing: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()